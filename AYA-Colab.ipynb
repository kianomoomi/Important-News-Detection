{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iugVwRWWui-X",
        "outputId": "0658edc9-bc30-47d0-dd74-803db42477e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBkhFlFYvadW"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mw73MBAWvgO8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "def get_k_most_similar_texts_by_tfidf(target_text, texts=None, k=5):\n",
        "    texts = []\n",
        "    file_path = \"/content/gdrive/MyDrive/News-Data/train.csv\"\n",
        "    df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "    for index, row in df.iterrows():\n",
        "        texts.append((row[1], row[2], row[4]))\n",
        "\n",
        "    # Initialize the vectorizer to include unigrams, bigrams, and trigrams\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
        "    text_vectors = vectorizer.fit_transform([text[0] for text in texts] + [target_text])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_similarities = cosine_similarity(text_vectors[-1], text_vectors[:-1])\n",
        "    cosine_similarities = cosine_similarities[0]  # Extract the first row from the 2D array\n",
        "\n",
        "    # Get indices of top k similar texts\n",
        "    top_indices = cosine_similarities.argsort()[::-1][:k]\n",
        "\n",
        "    # Return the top k similar texts and their similarities\n",
        "    results = [(texts[i][0], texts[i][-1], cosine_similarities[i]) for i in top_indices]\n",
        "    return results\n",
        "\n",
        "# Class of large language model used\n",
        "class AYA23Generator:\n",
        "    def __init__(self, model_name, quantize_4bit=True, use_flash_attention=False):\n",
        "        self.model_name = model_name\n",
        "        self.quantize_4bit = quantize_4bit\n",
        "        self.use_flash_attention = use_flash_attention\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        # quantization config that can be used if it is needed\n",
        "        quantization_config = None\n",
        "        if self.quantize_4bit:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "\n",
        "            )\n",
        "\n",
        "\n",
        "        attn_implementation = None\n",
        "        if self.use_flash_attention:\n",
        "            attn_implementation = \"flash_attention_2\"\n",
        "\n",
        "        # Loading the model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            attn_implementation=attn_implementation,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "        # Loading the tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        print(\"Model and tokenizer loaded successfully.\")\n",
        "\n",
        "    # Formatting the prompt in the desired format\n",
        "    def get_message_format(self, prompts):\n",
        "        return [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "\n",
        "    # Generating the output with the generation config provided\n",
        "    def generate_responses(self, prompts, temperature=0.3, top_p=0.75, top_k=0, max_new_tokens=1024):\n",
        "        messages = self.get_message_format(prompts)\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "        prompt_padded_len = len(input_ids[0])\n",
        "        gen_tokens = self.model.generate(\n",
        "            input_ids,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "        )\n",
        "        gen_tokens = [gt[prompt_padded_len:] for gt in gen_tokens]\n",
        "        return self.tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content')\n",
        "MODEL_NAME = \"CohereForAI/aya-23-8B\"\n",
        "generator = AYA23Generator(MODEL_NAME)\n",
        "test_df = pd.read_csv('/content/gdrive/MyDrive/News-Data/test.csv')\n",
        "aya_df = pd.read_csv('/content/gdrive/MyDrive/News-Data/results_aya.csv')\n",
        "column_name_to_write = 'predicted_k_50'\n",
        "start_row = aya_df.index[pd.isna(aya_df[column_name_to_write])].tolist()[0]\n",
        "for i in range(start_row, len(aya_df)):\n",
        "      # Specifying the k_shot value, and the correct prompt text accordingly\n",
        "      k_shot = int(column_name_to_write.split('_')[2])\n",
        "      if k_shot == 0:\n",
        "          prompt_fa_kshot = \"\"\"هدف، داشتن یک دسته‌بند دودویی است که با گرفتن هر متن ورودی، کلاس آن را در خروجی مشخص می‌کند. کلاس‌ها شامل دو دسته‌ی 1 یا 0 هستند. 1 یعنی خبر مهم است و 0 یعنی خبر مهم نیست.\n",
        "\n",
        "      شرح تسک:\n",
        "      متن یا خبری را مهم یا تاثیرگذار می‌گوییم اگر که برای بیش‌تر کاربران فارسی‌زبان اهمیت بالایی داشته باشد. یا به عبارت دیگر، جمعیت زیاد و بزرگی از ایرانیان مایل باشند که آن متن یا خبر را بخوانند و یا برای یکدیگر بفرستند. اگر خبری مربوط به یک قشر کوچک یا جامعه‌ی خاصی از کاربران باشد، آن خبر مهم نیست.\n",
        "      در صورتی که متن ورودی مهم باشد، کلاس 1 خواهد بود و در صورتی که مهم نباشد، کلاس 0 خواهد بود\n",
        "      برخی از مفاهیم مهم عبارت‌اند از:\n",
        "      یارانه و سهام و مواردی که قرار است پول به مردم برسد مهم هستند\n",
        "      ثبت نام مسکن و خانه و اخبار مربوط به وام‌ها و...\n",
        "      ثبت نام خودرو\n",
        "      افزایش و کاهش های شدید و زیاد قیمت ارز یا طلا و سکه و یا تورم\n",
        "\n",
        "      سیاسی:\n",
        "      اخبار جنگ، برجام، توافق های ایران،\n",
        "      تحریم های ایران،\n",
        "      خبرهای جنگ‌های بزرگ منطقه‌ای،\n",
        "      عزل و نصب مقامات بلندپایه ایرانی،\n",
        "      این‌ها همگی مهم هستند\n",
        "\n",
        "      ورزشی:\n",
        "      اخبار مربوط به تیم‌های معروف و پرطرفدار ایرانی و همین‌طور اروپایی مهم است\n",
        "\n",
        "        با توجه به متن زیر تنها در یک واژه پاسخ بده که باتوجه به مفاهیمی که در بالا مطرح شد و قدرت استنتاجی که خودت داری، آیا متن\n",
        "      مهم (تاثیرگذاری) حساب می‌شود یا خیر. (1 یا 0):\n",
        "      '''\n",
        "      ^^body^^\n",
        "      '''\n",
        "      در خروجی فقط مجاز هستی عدد ۱ یا عدد ۰ بنویسی. بدون هیچ توضیح اضافه‌ای.\n",
        "      \"\"\"\n",
        "      else:\n",
        "          prompt_fa_kshot = \"\"\"هدف، داشتن یک دسته‌بند دودویی است که با گرفتن هر متن ورودی، کلاس آن را در خروجی مشخص می‌کند. کلاس‌ها شامل دو دسته‌ی 1 یا 0 هستند. 1 یعنی خبر مهم است و 0 یعنی خبر مهم نیست.\n",
        "\n",
        "          شرح تسک:\n",
        "          متن یا خبری را مهم یا تاثیرگذار می‌گوییم اگر که برای بیش‌تر کاربران فارسی‌زبان اهمیت بالایی داشته باشد. یا به عبارت دیگر، جمعیت زیاد و بزرگی از ایرانیان مایل باشند که آن متن یا خبر را بخوانند و یا برای یکدیگر بفرستند. اگر خبری مربوط به یک قشر کوچک یا جامعه‌ی خاصی از کاربران باشد، آن خبر مهم نیست.\n",
        "          در صورتی که متن ورودی مهم باشد، کلاس 1 خواهد بود و در صورتی که مهم نباشد، کلاس 0 خواهد بود\n",
        "          برخی از مفاهیم مهم عبارت‌اند از:\n",
        "          یارانه و سهام و مواردی که قرار است پول به مردم برسد مهم هستند\n",
        "          ثبت نام مسکن و خانه و اخبار مربوط به وام‌ها و...\n",
        "          ثبت نام خودرو\n",
        "          افزایش و کاهش های شدید و زیاد قیمت ارز یا طلا و سکه و یا تورم\n",
        "\n",
        "          سیاسی:\n",
        "          اخبار جنگ، برجام، توافق های ایران،\n",
        "          تحریم های ایران،\n",
        "          خبرهای جنگ‌های بزرگ منطقه‌ای،\n",
        "          عزل و نصب مقامات بلندپایه ایرانی،\n",
        "          این‌ها همگی مهم هستند\n",
        "\n",
        "          ورزشی:\n",
        "          اخبار مربوط به تیم‌های معروف و پرطرفدار ایرانی و همین‌طور اروپایی مهم است\n",
        "\n",
        "          نمونه‌ها: چند نمونه پایین را ببین و باتوجه به آن‌ها به سوال پایین پاسخ بده\n",
        "          SAMPLES_HERE\n",
        "          از روی نمونه‌های بالایی یاد بگیر و خروجی را مشخص کن (فقط ۰ یا ۱).\n",
        "          حال  با توجه به «نمونه‌های بالا»، برای متن زیر تنها در یک واژه پاسخ بده که باتوجه به مفاهیمی که در بالا مطرح شد و قدرت استنتاجی که خودت داری، آیا متن\n",
        "          مهم (تاثیرگذاری) حساب می‌شود یا خیر. (1 یا 0):\n",
        "          '''\n",
        "          ^^body^^\n",
        "          '''\n",
        "          در خروجی فقط مجاز هستی عدد ۱ یا عدد ۰ بنویسی. بدون هیچ توضیح اضافه‌ای.\n",
        "          \"\"\"\n",
        "      test_df_counter = i % len(test_df)\n",
        "      print(f\"test_df_counter is {test_df_counter}\")\n",
        "      target_text = aya_df['text'][i]\n",
        "      if (len(target_text) > 10000):\n",
        "          target_text = target_text[:8000]\n",
        "    \n",
        "      # Placing the target text and similar news in their correct position in the prompt template\n",
        "      new_prompt = prompt_fa_kshot.replace(\"^^body^^\",  target_text)\n",
        "      if k_shot != 0:\n",
        "          sample_str = ''\n",
        "          for _ in range(k_shot):\n",
        "              sample_str += 'متن: {}\\n' +\\\n",
        "              'خروجی : {}\\n'\n",
        "          new_prompt = new_prompt.replace('SAMPLES_HERE', sample_str)\n",
        "          samples = []\n",
        "          similar_texts = get_k_most_similar_texts_by_tfidf(test_df['title'][test_df_counter] + '\\n' + test_df['text'][test_df_counter], k=k_shot)\n",
        "          for text in similar_texts:\n",
        "              samples.append(text[0])\n",
        "              samples.append(text[1])\n",
        "          new_prompt = new_prompt.format(*samples)\n",
        "      new_prompt = [new_prompt]\n",
        "      # Providing the prompt to the model and generating the output\n",
        "      result = int(generator.generate_responses(new_prompt)[0])\n",
        "      torch.cuda.empty_cache()\n",
        "      # Saving the output to the working dataframe\n",
        "      aya_df.at[i, column_name_to_write] = result\n",
        "      print(f\"answer of row {i} is {result} and k is {k_shot} and text type is {aya_df['text_type'][i]}\")\n",
        "      if i % 10 == 0:\n",
        "          aya_df.to_csv('/content/gdrive/MyDrive/News-Data/results_aya.csv', index=False)\n",
        "          print(f\"dataframe saved to csv file at iteration {i}\")\n",
        "\n",
        "aya_df.to_csv('/content/gdrive/MyDrive/News-Data/results_aya.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
